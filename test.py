import telebot
from telebot import types
import os
import re
import random
import docx2txt
from docx import Document
from settings import API_KEY

bot = telebot.TeleBot(API_KEY)

# –•—Ä–∞–Ω–∏–º –≤—Å—ë –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º
user_data = {}

# ---------- –ö–õ–ê–í–ò–ê–¢–£–†–ê ----------
def main_keyboard():
    kb = types.ReplyKeyboardMarkup(resize_keyboard=True)
    kb.row("üìò –ó–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏", "üß© –ó–∞–≥—Ä—É–∑–∏—Ç—å –≤–æ–ø—Ä–æ—Å—ã")
    kb.row("üóë –£–¥–∞–ª–∏—Ç—å –≤—Å–µ —Ñ–∞–π–ª—ã")
    kb.row("üß† –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ñ–∞–π–ª—ã")
    return kb


# ---------- START ----------
@bot.message_handler(commands=['start'])
def start(message):
    bot.send_message(
        message.chat.id,
        f"üëã –ü—Ä–∏–≤–µ—Ç, {message.from_user.first_name or '–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å'}!\n\n"
        "–Ø –±–æ—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ñ–∞–π–ª–æ–≤ –ø–æ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è–º üìÑ\n\n"
        "1Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª —Å –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è–º–∏ (.docx)\n"
        "2Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏ (.docx)\n"
        "3Ô∏è‚É£ –í–≤–µ–¥–∏—Ç–µ —á–∞—Å—Ç—å –Ω–∞–∑–≤–∞–Ω–∏—è –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä: *–∏–Ω–æ—Å—Ç—Ä*, *–∫–æ–º–∞–Ω–¥–Ω*, *–∏–Ω—Ñ–æ—Ä–º*)\n"
        "4Ô∏è‚É£ –ù–∞–∂–º–∏—Ç–µ üß† *–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ñ–∞–π–ª—ã*\n\n"
        "–Ø —Å–æ–∑–¥–∞–º Word-—Ñ–∞–π–ª—ã —Ç–æ–ª—å–∫–æ –ø–æ –Ω–∞–π–¥–µ–Ω–Ω—ã–º –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞–º üìò",
        parse_mode="Markdown",
        reply_markup=main_keyboard()
    )


# ---------- –¢–ï–ö–°–¢ ----------
@bot.message_handler(content_types=['text'])
def handle_text(message):
    user_id = message.from_user.id
    text = message.text.strip().lower()

    user_dir = f"data_{user_id}"
    comp_file = os.path.join(user_dir, "competencies.docx")
    quest_file = os.path.join(user_dir, "questions.docx")

    os.makedirs(user_dir, exist_ok=True)
    user_data.setdefault(user_id, {})

    # ---- –ó–ê–ì–†–£–ó–ö–ê ----
    if text == "üìò –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏":
        bot.send_message(message.chat.id, "üì§ –û—Ç–ø—Ä–∞–≤—å—Ç–µ Word-—Ñ–∞–π–ª (.docx) —Å –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è–º–∏.")
        user_data[user_id]["mode"] = "competencies"
        return

    if text == "üß© –∑–∞–≥—Ä—É–∑–∏—Ç—å –≤–æ–ø—Ä–æ—Å—ã":
        bot.send_message(message.chat.id, "üì§ –û—Ç–ø—Ä–∞–≤—å—Ç–µ Word-—Ñ–∞–π–ª (.docx) —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏.")
        user_data[user_id]["mode"] = "questions"
        return

    # ---- –£–î–ê–õ–ï–ù–ò–ï ----
    if text == "üóë —É–¥–∞–ª–∏—Ç—å –≤—Å–µ —Ñ–∞–π–ª—ã":
        if os.path.exists(user_dir):
            for f in os.listdir(user_dir):
                os.remove(os.path.join(user_dir, f))
            bot.send_message(message.chat.id, "‚úÖ –í—Å–µ —Ñ–∞–π–ª—ã —É–¥–∞–ª–µ–Ω—ã.", reply_markup=main_keyboard())
        else:
            bot.send_message(message.chat.id, "‚ö†Ô∏è –£ –≤–∞—Å –Ω–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤.", reply_markup=main_keyboard())
        return

    # ---- –ì–ï–ù–ï–†–ê–¶–ò–Ø ----
    if text == "üß† —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ñ–∞–π–ª—ã":
        data = user_data.get(user_id, {})
        found = data.get("found_disciplines")
        if not found:
            bot.send_message(message.chat.id, "‚ö†Ô∏è –°–Ω–∞—á–∞–ª–∞ –≤–≤–µ–¥–∏—Ç–µ —á–∞—Å—Ç—å –Ω–∞–∑–≤–∞–Ω–∏—è –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã, —á—Ç–æ–±—ã —è –Ω–∞—à—ë–ª –Ω—É–∂–Ω—ã–µ.")
            return
        if not os.path.exists(quest_file):
            bot.send_message(message.chat.id, "‚ö†Ô∏è –ù—É–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏ (.docx).")
            return

        bot.send_message(message.chat.id, "‚è≥ –ì–µ–Ω–µ—Ä–∏—Ä—É—é —Ñ–∞–π–ª—ã, –ø–æ–¥–æ–∂–¥–∏—Ç–µ...")

        competencies = data.get("competencies", {})
        questions, _ = extract_questions(quest_file)
        generated = generate_files_per_discipline(user_dir, found, competencies, questions)

        for file_path in generated:
            with open(file_path, "rb") as f:
                bot.send_document(message.chat.id, f)
        bot.send_message(message.chat.id, "‚úÖ –§–∞–π–ª—ã —É—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã!", reply_markup=main_keyboard())
        return

    # ---- –ü–û–ò–°–ö ----
    if not os.path.exists(comp_file):
        bot.send_message(message.chat.id, "‚ö†Ô∏è –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª —Å –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è–º–∏ (.docx)")
        return

    # –ï—Å–ª–∏ –µ—â—ë –Ω–µ –∏–∑–≤–ª–µ–∫–∞–ª–∏
    if "disciplines" not in user_data[user_id]:
        disciplines = extract_disciplines(comp_file)
        competencies = extract_competencies(comp_file)
        user_data[user_id]["disciplines"] = disciplines
        user_data[user_id]["competencies"] = competencies
        bot.send_message(
            message.chat.id,
            f"‚úÖ –§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω! –ù–∞–π–¥–µ–Ω–æ {len(disciplines)} –¥–∏—Å—Ü–∏–ø–ª–∏–Ω –∏ {len(competencies)} –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π.\n\n"
            "‚úèÔ∏è –¢–µ–ø–µ—Ä—å –Ω–∞–ø–∏—à–∏ —á–∞—Å—Ç—å –Ω–∞–∑–≤–∞–Ω–∏—è –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã, –Ω–∞–ø—Ä–∏–º–µ—Ä: *–∏–Ω–æ—Å—Ç—Ä*, *–∫–æ–º–∞–Ω–¥–Ω*, *–∏–Ω—Ñ–æ—Ä–º*.",
            parse_mode="Markdown"
        )
        return

    disciplines = user_data[user_id]["disciplines"]
    competencies = user_data[user_id]["competencies"]

    found = [d for d in disciplines if text in d.lower()]

    if not found:
        bot.send_message(message.chat.id, "‚ùå –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –≤–≤–µ—Å—Ç–∏ –¥—Ä—É–≥—É—é —á–∞—Å—Ç—å –Ω–∞–∑–≤–∞–Ω–∏—è.")
        return

    # --- –≤—ã–≤–æ–¥ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π ---
    bot.send_message(message.chat.id, "üìö –ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π:\n\n" + "\n\n".join([f"üìò {d}" for d in found]))

    # --- –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –ø–æ –Ω–∞–π–¥–µ–Ω–Ω—ã–º –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞–º ---
    response_lines = []
    for d in found:
        response_lines.append(f"üìò *{d}*")
        uk_codes = re.findall(r"–£–ö\s*\d+\.\d", d)
        if not uk_codes:
            response_lines.append("‚ö†Ô∏è –ù–µ—Ç –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π –¥–ª—è —ç—Ç–æ–π –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã.\n")
            continue
        for uk in uk_codes:
            uk_key = uk.replace(" ", "")
            if uk_key in competencies:
                response_lines.append(f"üìó {competencies[uk_key]}")
            else:
                response_lines.append(f"‚ö†Ô∏è {uk} ‚Äî –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
        response_lines.append("")

    bot.send_message(
        message.chat.id,
        "üìñ *–ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞–º–∏:*\n\n" + "\n".join(response_lines),
        parse_mode="Markdown",
        reply_markup=main_keyboard()
    )

    user_data[user_id]["found_disciplines"] = found


# ---------- –î–û–ö–£–ú–ï–ù–¢–´ ----------
@bot.message_handler(content_types=['document'])
def handle_document(message):
    user_id = message.from_user.id
    mode = user_data.get(user_id, {}).get("mode")
    if not mode:
        bot.send_message(message.chat.id, "‚ö†Ô∏è –°–Ω–∞—á–∞–ª–∞ –≤—ã–±–µ—Ä–∏—Ç–µ, —á—Ç–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å: –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –∏–ª–∏ –≤–æ–ø—Ä–æ—Å—ã.")
        return

    user_dir = f"data_{user_id}"
    os.makedirs(user_dir, exist_ok=True)
    file_path = os.path.join(user_dir, f"{mode}.docx")

    file_info = bot.get_file(message.document.file_id)
    downloaded = bot.download_file(file_info.file_path)
    with open(file_path, "wb") as f:
        f.write(downloaded)

    bot.send_message(message.chat.id, f"‚úÖ –§–∞–π–ª '{message.document.file_name}' —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω.", reply_markup=main_keyboard())


# ---------- –ü–ê–†–°–ï–†–´ ----------
def extract_disciplines(file_path):
    full_text = docx2txt.process(file_path)
    print("üìò –¢–µ–∫—Å—Ç —É—Å–ø–µ—à–Ω–æ —Å—á–∏—Ç–∞–Ω. –û–±—â–∞—è –¥–ª–∏–Ω–∞:", len(full_text))
    pattern = r"(–ë\d{1,2}[–ê-–ØA-Za-z–∞-—è—ë–Å]*\s*\d*\s*[–ê-–ØA-Za-z–∞-—è—ë–Å0-9,\-‚Äì\s]+?\(–£–ö\s*[\d.\s–ê-–Ø–∞-—èA-Za-z]*\))"
    matches = re.findall(pattern, full_text)
    print("üîç –ù–∞–π–¥–µ–Ω–æ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω:", len(matches))
    disciplines = [" ".join(m.split()) for m in matches]
    return disciplines


def extract_competencies(file_path):
    full_text = docx2txt.process(file_path)
    full_text = re.sub(r"\s+", " ", full_text)
    pattern = r"(–£–ö\s*\d+\.\d)(?:\s*[‚Äì-]?\s*)([^–£–ë]+)"
    matches = re.findall(pattern, full_text)
    competencies = {}
    for code, desc in matches:
        clean_code = code.replace(" ", "")
        clean_desc = desc.strip()
        if len(clean_desc) < 10 or "–£–ö" in clean_desc[:10]:
            continue
        if len(clean_desc) > 400:
            clean_desc = clean_desc[:400].rsplit('.', 1)[0] + "..."
        competencies[clean_code] = f"{code} ‚Äî {clean_desc}"
    print("üìò –ù–∞–π–¥–µ–Ω–æ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π:", len(competencies))
    return competencies


def extract_questions(file_path):
    text = docx2txt.process(file_path)
    text = re.sub(r'[ \t]+', ' ', text)
    text = re.sub(r'\n{2,}', '\n\n', text)

    sections = [
        "–ï–í", "–ú–í", "–ß–í", "–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ",
        "–û–¥–Ω–æ –ø—Ä–æ–ø—É—â–µ–Ω–Ω–æ–µ —Å–ª–æ–≤–æ", "–î–≤–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤–∞", "–í–ª–æ–∂–µ–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã"
    ]

    categorized, current = {}, None
    for line in text.splitlines():
        stripped = line.strip()
        if stripped in sections:
            current = stripped
            categorized[current] = ""
        elif current:
            categorized[current] += line + "\n"

    def normalize_options(options):
        opts = [o.strip() for o in options.splitlines() if o.strip()]
        return "\n".join(opts[:4])

    def find_ev(text):
        matches = re.findall(r"([^\n]+?\?)\s*\n((?:[^\n]*\n){2,8})", text, re.DOTALL)
        return [(q.strip(), normalize_options(o)) for q, o in matches]

    def find_mv(text):
        matches = re.findall(r"([^\n]+?\?)\s*\n((?:[^\n]*\n){2,8})", text, re.DOTALL)
        return [(q.strip(), normalize_options(o)) for q, o in matches]

    def find_chv(text):
        return re.findall(r"([^\n]+?\(–í–≤–µ–¥–∏—Ç–µ[^\n]+?\))\s*\n\s*=\s*([^\n]+)", text, re.DOTALL)

    def find_matching(text):
        blocks = re.findall(r"(–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ.+?(?=(?:\n–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ|$)))", text, re.DOTALL)
        return [re.sub(r'\n{2,}', '\n', b).strip() for b in blocks]

    def find_one_gap(text):
        return re.findall(r"([^\n]+?\(–í–≤–µ–¥–∏—Ç–µ[^\n]+?\))", text)

    def find_two_gap(text):
        blocks = re.split(r'(?=\n?.*?\[\[1\]\].*?\[\[2\]\])', text)
        results = []
        for block in blocks:
            block = block.strip()
            if not block or '[[1]]' not in block:
                continue
            main_part_match = re.search(r'([^\n]*\[\[1\]\].+?\[\[2\]\][^\n]*)', block)
            if not main_part_match:
                continue
            main_part = main_part_match.group(1).strip()
            opt_match = re.search(
                r'(1\s*=\s*[^\n]+(?:\n\s*(?!\d=)[^\n]+)*\n\s*2\s*=\s*[^\n]+(?:\n\s*(?!\[\[)[^\n]+)*)',
                block,
                re.DOTALL
            )
            options = ""
            if opt_match:
                options = "\n" + re.sub(r'\n{2,}', '\n', opt_match.group(1)).strip()
            results.append(f"{main_part}\n{options}".strip())
        return results

    def find_nested(text):
        blocks = re.findall(r"(?:\s*\d+\s*\n)?(.+?(?=\n\s*\d+\s*\n|$))", text, re.DOTALL)
        return [re.sub(r'\n{2,}', '\n', b).strip() for b in blocks if b.strip()]

    extractors = {
        "–ï–í": find_ev, "–ú–í": find_mv, "–ß–í": find_chv,
        "–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ": find_matching,
        "–û–¥–Ω–æ –ø—Ä–æ–ø—É—â–µ–Ω–Ω–æ–µ —Å–ª–æ–≤–æ": find_one_gap,
        "–î–≤–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤–∞": find_two_gap,
        "–í–ª–æ–∂–µ–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã": find_nested
    }

    questions = []
    for key, func in extractors.items():
        sec = categorized.get(key, "")
        if not sec.strip():
            continue
        found = func(sec)
        for q in found:
            if isinstance(q, tuple):
                q_text = f"{q[0]}\n{q[1]}"
            else:
                q_text = str(q)
            questions.append(q_text.strip())

    return questions, None


# ---------- –ì–ï–ù–ï–†–ê–¶–ò–Ø ----------
def generate_files_per_discipline(user_dir, disciplines, competencies, questions):
    generated = []
    for disc in disciplines:
        doc = Document()
        doc.add_heading(disc, level=1)
        uk_codes = re.findall(r"(–£–ö\s*\d+\.\d)", disc)

        for uk in uk_codes:
            uk_key = uk.replace(" ", "")
            if uk_key in competencies:
                doc.add_paragraph(f"üìó {competencies[uk_key]}", style='List Bullet')
                selected = random.sample(questions, min(15, len(questions)))
                for i, q in enumerate(selected, 1):
                    doc.add_paragraph(f"{i}. {q}", style='List Number')

        filename = re.sub(r'[^A-Za-z–ê-–Ø–∞-—è0-9]', '_', disc[:40]) + ".docx"
        file_path = os.path.join(user_dir, filename)
        doc.save(file_path)
        generated.append(file_path)

    return generated


# ---------- MAIN ----------
if __name__ == "__main__":
    print("ü§ñ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω: –ø–æ–∏—Å–∫ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ –Ω–∞–π–¥–µ–Ω–Ω—ã–º –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞–º")
    bot.polling(none_stop=True)
