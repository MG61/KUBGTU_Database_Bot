import telebot
from telebot import types
import os
import re
import random
import docx2txt
from docx import Document
from settings import API_KEY
from docx.shared import Pt
from docx.oxml.ns import qn

bot = telebot.TeleBot(API_KEY)

# –•—Ä–∞–Ω–∏–º –≤—Å—ë –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º
user_data = {}

# ---------- –ö–õ–ê–í–ò–ê–¢–£–†–ê ----------
def main_keyboard():
    kb = types.ReplyKeyboardMarkup(resize_keyboard=True)
    kb.row("üìò –ó–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏", "üß© –ó–∞–≥—Ä—É–∑–∏—Ç—å –≤–æ–ø—Ä–æ—Å—ã")
    kb.row("üóë –£–¥–∞–ª–∏—Ç—å –≤—Å–µ —Ñ–∞–π–ª—ã")
    kb.row("üß† –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ñ–∞–π–ª—ã")
    return kb


# ---------- START ----------
@bot.message_handler(commands=['start'])
def start(message):
    bot.send_message(
        message.chat.id,
        f"üëã –ü—Ä–∏–≤–µ—Ç, {message.from_user.first_name or '–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å'}!\n\n"
        "–Ø –±–æ—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ñ–∞–π–ª–æ–≤ –ø–æ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è–º üìÑ\n\n"
        "1Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª —Å –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è–º–∏ (.docx)\n"
        "2Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏ (.docx)\n"
        "3Ô∏è‚É£ –í–≤–µ–¥–∏—Ç–µ —á–∞—Å—Ç—å –Ω–∞–∑–≤–∞–Ω–∏—è –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä: *–∏–Ω–æ—Å—Ç—Ä*, *–∫–æ–º–∞–Ω–¥–Ω*, *–∏–Ω—Ñ–æ—Ä–º*)\n"
        "4Ô∏è‚É£ –ù–∞–∂–º–∏—Ç–µ üß† *–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ñ–∞–π–ª—ã*\n\n"
        "–Ø —Å–æ–∑–¥–∞–º Word-—Ñ–∞–π–ª—ã —Ç–æ–ª—å–∫–æ –ø–æ –Ω–∞–π–¥–µ–Ω–Ω—ã–º –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞–º üìò",
        parse_mode="Markdown",
        reply_markup=main_keyboard()
    )


# ---------- –¢–ï–ö–°–¢ ----------
@bot.message_handler(content_types=['text'])
def handle_text(message):
    user_id = message.from_user.id
    text = message.text.strip().lower()

    user_dir = f"data_{user_id}"
    comp_file = os.path.join(user_dir, "competencies.docx")
    quest_file = os.path.join(user_dir, "questions.docx")

    os.makedirs(user_dir, exist_ok=True)
    user_data.setdefault(user_id, {})

    # ---- –ó–ê–ì–†–£–ó–ö–ê ----
    if text == "üìò –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏":
        bot.send_message(message.chat.id, "üì§ –û—Ç–ø—Ä–∞–≤—å—Ç–µ Word-—Ñ–∞–π–ª (.docx) —Å –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è–º–∏.")
        user_data[user_id]["mode"] = "competencies"
        return

    if text == "üß© –∑–∞–≥—Ä—É–∑–∏—Ç—å –≤–æ–ø—Ä–æ—Å—ã":
        bot.send_message(message.chat.id, "üì§ –û—Ç–ø—Ä–∞–≤—å—Ç–µ Word-—Ñ–∞–π–ª (.docx) —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏.")
        user_data[user_id]["mode"] = "questions"
        return

    # ---- –£–î–ê–õ–ï–ù–ò–ï ----
    if text == "üóë —É–¥–∞–ª–∏—Ç—å –≤—Å–µ —Ñ–∞–π–ª—ã":
        if os.path.exists(user_dir):
            for f in os.listdir(user_dir):
                os.remove(os.path.join(user_dir, f))
            bot.send_message(message.chat.id, "‚úÖ –í—Å–µ —Ñ–∞–π–ª—ã —É–¥–∞–ª–µ–Ω—ã.", reply_markup=main_keyboard())
        else:
            bot.send_message(message.chat.id, "‚ö†Ô∏è –£ –≤–∞—Å –Ω–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤.", reply_markup=main_keyboard())
        return

    # ---- –ì–ï–ù–ï–†–ê–¶–ò–Ø ----
    if text == "üß† —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ñ–∞–π–ª—ã":
        data = user_data.get(user_id, {})
        found = data.get("found_disciplines")
        if not found:
            bot.send_message(message.chat.id, "‚ö†Ô∏è –°–Ω–∞—á–∞–ª–∞ –≤–≤–µ–¥–∏—Ç–µ —á–∞—Å—Ç—å –Ω–∞–∑–≤–∞–Ω–∏—è –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã, —á—Ç–æ–±—ã —è –Ω–∞—à—ë–ª –Ω—É–∂–Ω—ã–µ.")
            return
        if not os.path.exists(quest_file):
            bot.send_message(message.chat.id, "‚ö†Ô∏è –ù—É–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏ (.docx).")
            return

        bot.send_message(message.chat.id, "‚è≥ –ì–µ–Ω–µ—Ä–∏—Ä—É—é —Ñ–∞–π–ª—ã, –ø–æ–¥–æ–∂–¥–∏—Ç–µ...")

        competencies = data.get("competencies", {})
        questions, _ = extract_questions(quest_file)
        generated = generate_files_per_discipline(user_dir, found, competencies, questions)

        for file_path in generated:
            with open(file_path, "rb") as f:
                bot.send_document(message.chat.id, f)
        bot.send_message(message.chat.id, "‚úÖ –§–∞–π–ª—ã —É—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã!", reply_markup=main_keyboard())
        return

    # ---- –ü–û–ò–°–ö ----
    if not os.path.exists(comp_file):
        bot.send_message(message.chat.id, "‚ö†Ô∏è –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª —Å –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è–º–∏ (.docx)")
        return

    # –ï—Å–ª–∏ –µ—â—ë –Ω–µ –∏–∑–≤–ª–µ–∫–∞–ª–∏
    if "disciplines" not in user_data[user_id]:
        disciplines = extract_disciplines(comp_file)
        competencies = extract_competencies(comp_file)
        user_data[user_id]["disciplines"] = disciplines
        user_data[user_id]["competencies"] = competencies
        bot.send_message(
            message.chat.id,
            f"‚úÖ –§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω! –ù–∞–π–¥–µ–Ω–æ {len(disciplines)} –¥–∏—Å—Ü–∏–ø–ª–∏–Ω –∏ {len(competencies)} –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π.\n\n"
            "‚úèÔ∏è –¢–µ–ø–µ—Ä—å –Ω–∞–ø–∏—à–∏ —á–∞—Å—Ç—å –Ω–∞–∑–≤–∞–Ω–∏—è –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã, –Ω–∞–ø—Ä–∏–º–µ—Ä: *–∏–Ω–æ—Å—Ç—Ä*, *–∫–æ–º–∞–Ω–¥–Ω*, *–∏–Ω—Ñ–æ—Ä–º*.",
            parse_mode="Markdown"
        )
        return

    disciplines = user_data[user_id]["disciplines"]
    competencies = user_data[user_id]["competencies"]

    found = [d for d in disciplines if text in d.lower()]

    if not found:
        bot.send_message(message.chat.id, "‚ùå –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –≤–≤–µ—Å—Ç–∏ –¥—Ä—É–≥—É—é —á–∞—Å—Ç—å –Ω–∞–∑–≤–∞–Ω–∏—è.")
        return

    # --- –≤—ã–≤–æ–¥ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π ---
    send_long_message(message.chat.id, "üìö –ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π:\n\n" + "\n\n".join([f"üìò {d}" for d in found]))

    # --- –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –ø–æ –Ω–∞–π–¥–µ–Ω–Ω—ã–º –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞–º ---
    response_lines = []
    for d in found:
        response_lines.append(f"üìò *{d}*")
        # –∏—â–µ–º –≤—Å–µ —Ç–∏–ø—ã –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π (–£–ö, –û–ü–ö, –ü–ö) ‚Äî –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –≥–ª—É–±–∏–Ω—ã –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ (–Ω–∞–ø—Ä. 5.3.1)
        comp_codes = re.findall(r"(?:–£–ö|–û–ü–ö|–ü–ö)\s*\d+(?:\.\d+)*", d)
        if not comp_codes:
            response_lines.append("‚ö†Ô∏è –ù–µ—Ç –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π –¥–ª—è —ç—Ç–æ–π –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã.\n")
            continue
        for comp in comp_codes:
            comp_key = comp.replace(" ", "")
            desc = find_comp_desc(comp_key, competencies)
            if desc:
                response_lines.append(f"üìó {desc}")
            else:
                response_lines.append(f"‚ö†Ô∏è {comp} ‚Äî –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
        response_lines.append("")

    send_long_message(
        message.chat.id,
        "üìñ *–ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞–º–∏:*\n\n" + "\n".join(response_lines),
        parse_mode="Markdown",
        reply_markup=main_keyboard()
    )

    user_data[user_id]["found_disciplines"] = found


# ---------- –î–û–ö–£–ú–ï–ù–¢–´ ----------
@bot.message_handler(content_types=['document'])
def handle_document(message):
    user_id = message.from_user.id
    mode = user_data.get(user_id, {}).get("mode")

    if not mode:
        bot.send_message(message.chat.id, "‚ö†Ô∏è –°–Ω–∞—á–∞–ª–∞ –≤—ã–±–µ—Ä–∏—Ç–µ, —á—Ç–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å: –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –∏–ª–∏ –≤–æ–ø—Ä–æ—Å—ã.")
        return

    user_dir = f"data_{user_id}"
    os.makedirs(user_dir, exist_ok=True)
    file_path = os.path.join(user_dir, f"{mode}.docx")

    file_info = bot.get_file(message.document.file_id)
    downloaded = bot.download_file(file_info.file_path)
    with open(file_path, "wb") as f:
        f.write(downloaded)

    bot.send_message(
        message.chat.id,
        f"‚úÖ –§–∞–π–ª '{message.document.file_name}' —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω.",
        reply_markup=main_keyboard()
    )

    comp_file = os.path.join(user_dir, "competencies.docx")
    quest_file = os.path.join(user_dir, "questions.docx")

    # ‚úÖ –ï—Å–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –æ–±–∞ —Ñ–∞–π–ª–∞ ‚Äî —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ä–∞–∑ –ø–∞—Ä—Å–∏–º
    if os.path.exists(comp_file) and os.path.exists(quest_file):
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –±—ã–ª–∏ –ª–∏ —É–∂–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω—ã
        if "disciplines" not in user_data[user_id] or "competencies" not in user_data[user_id]:
            bot.send_message(message.chat.id, "‚è≥ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Ñ–∞–π–ª—ã, –ø–æ–¥–æ–∂–¥–∏—Ç–µ...")

            disciplines = extract_disciplines(comp_file)
            competencies = extract_competencies(comp_file)

            user_data[user_id]["disciplines"] = disciplines
            user_data[user_id]["competencies"] = competencies

            bot.send_message(
                message.chat.id,
                f"‚úÖ –§–∞–π–ª—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã!\n–ù–∞–π–¥–µ–Ω–æ {len(disciplines)} –¥–∏—Å—Ü–∏–ø–ª–∏–Ω –∏ {len(competencies)} –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π.\n\n"
                "‚úèÔ∏è –¢–µ–ø–µ—Ä—å –Ω–∞–ø–∏—à–∏ —á–∞—Å—Ç—å –Ω–∞–∑–≤–∞–Ω–∏—è –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã, –Ω–∞–ø—Ä–∏–º–µ—Ä: *–∏–Ω–æ—Å—Ç—Ä*, *–∫–æ–º–∞–Ω–¥–Ω*, *–∏–Ω—Ñ–æ—Ä–º.*",
                parse_mode="Markdown",
                reply_markup=main_keyboard()
            )
        else:
            # –ï—Å–ª–∏ —É–∂–µ –ø–∞—Ä—Å–∏–ª–∏ ‚Äî –ø—Ä–æ—Å—Ç–æ –Ω–∞–ø–æ–º–∏–Ω–∞–µ–º, —á—Ç–æ –¥–µ–ª–∞—Ç—å –¥–∞–ª—å—à–µ
            bot.send_message(
                message.chat.id,
                "‚úèÔ∏è –¢–µ–ø–µ—Ä—å –Ω–∞–ø–∏—à–∏ —á–∞—Å—Ç—å –Ω–∞–∑–≤–∞–Ω–∏—è –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã, –Ω–∞–ø—Ä–∏–º–µ—Ä: *–∏–Ω–æ—Å—Ç—Ä*, *–∫–æ–º–∞–Ω–¥–Ω*, *–∏–Ω—Ñ–æ—Ä–º.*",
                parse_mode="Markdown",
                reply_markup=main_keyboard()
            )


# ---------- –ü–ê–†–°–ï–†–´ ----------
def extract_disciplines(file_path):
    full_text = docx2txt.process(file_path)
    print("üìò –¢–µ–∫—Å—Ç —É—Å–ø–µ—à–Ω–æ —Å—á–∏—Ç–∞–Ω. –û–±—â–∞—è –¥–ª–∏–Ω–∞:", len(full_text))
    # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º –£–ö, –û–ü–ö –∏ –ü–ö –≤ —Å–∫–æ–±–∫–∞—Ö —É –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã
    pattern = r"(–ë\d{1,2}[–ê-–ØA-Za-z–∞-—è—ë–Å]*\s*\d*\s*[–ê-–ØA-Za-z–∞-—è—ë–Å0-9,\-‚Äì\s]+?\((?:–£–ö|–û–ü–ö|–ü–ö)\s*[\d.\s–ê-–Ø–∞-—èA-Zazl—ë–Å]*\))"
    matches = re.findall(pattern, full_text)
    print("üîç –ù–∞–π–¥–µ–Ω–æ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω:", len(matches))
    disciplines = [" ".join(m.split()) for m in matches]
    return disciplines


def extract_competencies(file_path):
    full_text = docx2txt.process(file_path)
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–µ–≤–æ–¥—ã —Å—Ç—Ä–æ–∫, –Ω–æ —É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã/—Ç–∞–±—É–ª—è—Ü–∏–∏
    cleaned = full_text.replace('\r', '')
    cleaned = re.sub(r"[ \t]+", " ", cleaned).strip()

    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –∫–æ–¥—ã: –£–ö, –û–ü–ö, –ü–ö —Å –æ–¥–Ω–æ–π –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Ç–æ—á–∫–∞–º–∏ –≤ –Ω–æ–º–µ—Ä–µ (–Ω–∞–ø—Ä. 5.3 –∏–ª–∏ 5.3.1 –∏ —Ç.–¥.)
    code_re = re.compile(r"((?:–£–ö|–û–ü–ö|–ü–ö)\s*\d+(?:\.\d+)*)")
    matches = list(code_re.finditer(cleaned))

    # –®–∞–±–ª–æ–Ω—ã, —É–∫–∞–∑—ã–≤–∞—é—â–∏–µ –Ω–∞ –≥—Ä–∞–Ω–∏—Ü—ã –±–ª–æ–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —è–≤–ª—è—é—Ç—Å—è –æ–ø–∏—Å–∞–Ω–∏–µ–º –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏
    stop_patterns = [
        r"\n\s*–ë\d",      # —Å–ª–µ–¥—É—é—â–∞—è –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –ë1...
        r"\n\s*‚Ññ\s",      # —Ç–∞–±–ª–∏—á–Ω–∞—è –Ω—É–º–µ—Ä–∞—Ü–∏—è/–∑–∞–≥–æ–ª–æ–≤–æ–∫
        r"–ö–æ–¥ –∏ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", r"\b–î–∏—Å—Ü–∏–ø–ª–∏–Ω—ã\b", r"\b–§–ì–û–°\b",
        r"\b–ü–°\b", r"\b–ë3–ì–ò–ê\b", r"\b–î–∏—Ä–µ–∫—Ç–æ—Ä\b", r"\b–ó–∞–≤–µ–¥—É—é—â–∏–π\b",
        r"\b–ü—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å\b", r"\b–°–≤—è–∑—å —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º–∏\b"
    ]

    # –ø–æ–¥—Å—Ç—Ä–æ–∫–∏ –¥–ª—è —É—Å–µ—á–µ–Ω–∏—è –æ–ø–∏—Å–∞–Ω–∏—è
    stop_subs = [
        '\n–ë', '\n‚Ññ', '‚Ññ ', '–ö–æ–¥ –∏ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', '–î–∏—Å—Ü–∏–ø–ª–∏–Ω—ã', '–§–ì–û–°', '–ü–° ', '–ë3–ì–ò–ê',
        '–î–∏—Ä–µ–∫—Ç–æ—Ä', '–ó–∞–≤–µ–¥—É—é—â–∏–π', '–ü—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å', '–°–≤—è–∑—å —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º–∏', '–ü–ö-', '–£–ö-', '–û–ü–ö-'
    ]

    competencies = {}
    for i, m in enumerate(matches):
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–π –∫–æ–¥: —É–±–∏—Ä–∞–µ–º –∑–∞–≤–µ—Ä—à–∞—é—â–∏–µ —Ç–æ—á–∫–∏/–∑–∞–ø—è—Ç—ã–µ/—Å–∫–æ–±–∫–∏
        code_text_raw = m.group(1)
        code_text = re.sub(r"[\.,;:\)\]]+$", "", code_text_raw).strip()

        start = m.end()
        next_code_start = matches[i + 1].start() if i + 1 < len(matches) else len(cleaned)
        end = next_code_start

        # –ò—â–µ–º –±–ª–∏–∂–∞–π—à–∏–π –º–∞—Ä–∫–µ—Ä-—Å—Ç–æ–ø —Å—Ä–µ–¥–∏ —Å—Ç–æ–ø-—à–∞–±–ª–æ–Ω–æ–≤
        for pat in stop_patterns:
            mm = re.search(pat, cleaned[start:next_code_start])
            if mm:
                candidate = start + mm.start()
                if candidate < end:
                    end = candidate

        # –¢–∞–∫–∂–µ –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞ –¥–≤–æ–π–Ω–æ–º –ø–µ—Ä–µ–≤–æ–¥–µ —Å—Ç—Ä–æ–∫–∏ (–Ω–æ–≤—ã–π –±–ª–æ–∫)
        mm = re.search(r"\n\s*\n", cleaned[start:next_code_start])
        if mm:
            candidate = start + mm.start()
            if candidate < end:
                end = candidate

        # –ü–æ–ø—Ä–æ–±—É–µ–º –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è –Ω–∞ –ø–µ—Ä–≤–æ–º –∫–æ–Ω—Ü–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö —Ä–∞–∑—É–º–Ω–æ–≥–æ (200 —Å–∏–º–≤–æ–ª–æ–≤)
        snippet = cleaned[start:end]
        sent = re.search(r"([\.\!?])\s+", snippet)
        if sent and sent.start() < 200:
            end = start + sent.end()

        desc_raw = cleaned[start:end].strip()

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è: —É–±–∏—Ä–∞–µ–º –≤–µ–¥—É—â–∏–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ –∏ –≤–∫—Ä–∞–ø–ª–µ–Ω–∏—è –∫–æ–¥–æ–≤
        desc_raw = re.sub(r"^[\s:;\-‚Äì‚Äî]+", "", desc_raw)
        desc_raw = re.sub(code_re, "", desc_raw).strip()

        # –£—Å–µ—á—ë–º –ø–æ –ø–µ—Ä–≤—ã–º —Å—Ç–æ–ø-–ø–æ–¥—Å—Ç—Ä–æ–∫–∞–º, —á—Ç–æ–±—ã —É–±—Ä–∞—Ç—å –≤–∫—Ä–∞–ø–ª–µ–Ω–∏—è —Ç–∞–±–ª–∏—Ü/–∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        earliest = None
        for s in stop_subs:
            idx = desc_raw.find(s)
            if idx != -1:
                if earliest is None or idx < earliest:
                    earliest = idx
        if earliest is not None:
            desc_raw = desc_raw[:earliest].strip()

        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Å—Ç—Ä–æ–∫–∞–º –∏ —É–±–∏—Ä–∞–µ–º —Å—Ç—Ä–æ–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã–≥–ª—è–¥—è—Ç –∫–∞–∫ –∑–∞–≥–æ–ª–æ–≤–∫–∏/–Ω–æ–º–µ—Ä–∞
        lines = [ln.strip() for ln in desc_raw.splitlines() if ln.strip()]
        clean_lines = []
        for ln in lines:
            if re.match(r"^(?:–ë\d|‚Ññ\s|–ö–æ–¥ –∏ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ|–î–∏—Å—Ü–∏–ø–ª–∏–Ω—ã|–§–ì–û–°|–ü–°\b|–ë3–ì–ò–ê|–î–∏—Ä–µ–∫—Ç–æ—Ä|–ó–∞–≤–µ–¥—É—é—â–∏–π|–ü—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å|–°–≤—è–∑—å —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º–∏|–ü–ö-|–£–ö-|–û–ü–ö-)", ln):
                break
            clean_lines.append(ln)
        desc_raw = ' '.join(clean_lines).strip()

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —É—Å–µ—á–∫–∞ –ø–æ —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–º—Å—è –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–∞–º (–∑–∞–∫—Ä—ã–≤–∞—é—â–∞—è —Å–∫–æ–±–∫–∞ + —Å–ª–µ–¥—É—é—â–∏–π –±–ª–æ–∫, —ç–º–æ–¥–∑–∏ –∏ —Ç.–ø.)
        artifact_patterns = [r"\)\s*–ë\d", r"\)\s*–ë", r"\)\s*‚Ññ", r"üìò", r"üìó", r"‚ö†Ô∏è", r"‚Ññ\s*–ö–æ–¥", r"–§–ì–û–°", r"–ü–°\s*\d", r"–ë3–ì–ò–ê"]
        earliest_art = None
        for ap in artifact_patterns:
            a = re.search(ap, desc_raw)
            if a:
                if earliest_art is None or a.start() < earliest_art:
                    earliest_art = a.start()
        if earliest_art is not None:
            desc_raw = desc_raw[:earliest_art].strip()

        # –£–±–∏—Ä–∞–µ–º –∑–∞–≤–µ—Ä—à–∞—é—â–∏–µ —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –∏ –æ–¥–∏–Ω–æ—á–Ω—ã–µ —Å–∫–æ–±–∫–∏
        desc_raw = re.sub(r"[\-‚Äì‚Äî\)\(\[\]:;\.,]+$", "", desc_raw).strip()

        # –§–æ–ª–±–µ–∫: –µ—Å–ª–∏ –æ–ø–∏—Å–∞–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ, –≤–æ–∑—å–º—ë–º —á—É—Ç—å –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç –¥–æ –±–ª–∏–∂–∞–π—à–µ–≥–æ –ª–æ–≥–∏—á–Ω–æ–≥–æ –∫–æ–Ω—Ü–∞
        if len(re.sub(r"\s+", "", desc_raw)) < 8:
            extra_end = min(len(cleaned), start + 400)
            candidate_block = cleaned[start:extra_end]
            # –æ–±—Ä–µ–∑–∞–µ–º candidate_block –ø–æ —Å—Ç–æ–ø-–ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
            for pat in stop_patterns:
                mm = re.search(pat, candidate_block)
                if mm:
                    candidate_block = candidate_block[:mm.start()]
            candidate_block = re.sub(code_re, "", candidate_block).strip()
            # —Ç–∞–∫–∂–µ —É–±–µ—Ä—ë–º —Å—Ç–æ–ø-–ø–æ–¥—Å—Ç—Ä–æ–∫–∏
            earliest2 = None
            for s in stop_subs:
                idx = candidate_block.find(s)
                if idx != -1:
                    if earliest2 is None or idx < earliest2:
                        earliest2 = idx
            if earliest2 is not None:
                candidate_block = candidate_block[:earliest2].strip()
            # –∏ —É—Å–µ—á—ë–º –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –≤ candidate_block
            earliest_art2 = None
            for ap in artifact_patterns:
                a = re.search(ap, candidate_block)
                if a:
                    if earliest_art2 is None or a.start() < earliest_art2:
                        earliest_art2 = a.start()
            if earliest_art2 is not None:
                candidate_block = candidate_block[:earliest_art2].strip()
            if len(re.sub(r"\s+", "", candidate_block)) >= 8:
                desc_raw = candidate_block

        # –û—Ç–±—Ä–∞—Å—ã–≤–∞–µ–º —è–≤–Ω–æ –º—É—Å–æ—Ä–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è (–Ω–µ—Ç –±—É–∫–≤)
        if not re.search(r"[–ê-–Ø–∞-—èA-Za-z]", desc_raw):
            continue

        # –û–±—Ä–µ–∑–∞–µ–º –ª–∏—à–Ω—é—é –¥–ª–∏–Ω—É
        if len(desc_raw) > 400:
            desc_raw = desc_raw[:400].rsplit('.', 1)[0] + "..."

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫–ª—é—á (—É–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –º–µ–∂–¥—É –ø—Ä–µ—Ñ–∏–∫—Å–æ–º –∏ —Ü–∏—Ñ—Ä–∞–º–∏)
        key = code_text.replace(" ", "")

        competencies[key] = f"{code_text} ‚Äî {desc_raw}"

    print("üìò –ù–∞–π–¥–µ–Ω–æ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π:", len(competencies))
    return competencies


def extract_questions(file_path):
    text = docx2txt.process(file_path)
    text = re.sub(r'[ \t]+', ' ', text)
    text = re.sub(r'\n{2,}', '\n\n', text)

    sections = [
        "–ï–í", "–ú–í", "–ß–í", "–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ",
        "–û–¥–Ω–æ –ø—Ä–æ–ø—É—â–µ–Ω–Ω–æ–µ —Å–ª–æ–≤–æ", "–î–≤–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤–∞", "–í–ª–æ–∂–µ–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã"
    ]

    categorized, current = {}, None
    for line in text.splitlines():
        stripped = line.strip()
        if stripped in sections:
            current = stripped
            categorized[current] = ""
        elif current:
            categorized[current] += line + "\n"

    def normalize_options(options):
        opts = [o.strip() for o in options.splitlines() if o.strip()]
        return "\n".join(opts[:4])

    def find_ev(text):
        matches = re.findall(r"([^\n]+?\?)\s*\n((?:[^\n]*\n){2,8})", text, re.DOTALL)
        return [(q.strip(), normalize_options(o)) for q, o in matches]

    def find_mv(text):
        matches = re.findall(r"([^\n]+?\?)\s*\n((?:[^\n]*\n){2,8})", text, re.DOTALL)
        return [(q.strip(), normalize_options(o)) for q, o in matches]

    def find_chv(text):
        return re.findall(r"([^\n]+?\(–í–≤–µ–¥–∏—Ç–µ[^\n]+?\))\s*\n\s*=\s*([^\n]+)", text, re.DOTALL)

    def find_matching(text):
        blocks = re.findall(r"(–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ.+?(?=(?:\n–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ|$)))", text, re.DOTALL)
        return [re.sub(r'\n{2,}', '\n', b).strip() for b in blocks]

    def find_one_gap(text):
        return re.findall(r"([^\n]+?\(–í–≤–µ–¥–∏—Ç–µ[^\n]+?\))", text)

    def find_two_gap(text):
        blocks = re.split(r'(?=\n?.*?\[\[1\]\].*?\[\[2\]\])', text)
        results = []
        for block in blocks:
            block = block.strip()
            if not block or '[[1]]' not in block:
                continue
            main_part_match = re.search(r'([^\n]*\[\[1\]\].+?\[\[2\]\][^\n]*)', block)
            if not main_part_match:
                continue
            main_part = main_part_match.group(1).strip()
            opt_match = re.search(
                r'(1\s*=\s*[^\n]+(?:\n\s*(?!\d=)[^\n]+)*\n\s*2\s*=\s*[^\n]+(?:\n\s*(?!\[\[)[^\n]+)*)',
                block,
                re.DOTALL
            )
            options = ""
            if opt_match:
                options = "\n" + re.sub(r'\n{2,}', '\n', opt_match.group(1)).strip()
            results.append(f"{main_part}\n{options}".strip())
        return results

    def find_nested(text):
        blocks = re.findall(r"(?:\s*\d+\s*\n)?(.+?(?=\n\s*\d+\s*\n|$))", text, re.DOTALL)
        return [re.sub(r'\n{2,}', '\n', b).strip() for b in blocks if b.strip()]

    extractors = {
        "–ï–í": find_ev, "–ú–í": find_mv, "–ß–í": find_chv,
        "–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ": find_matching,
        "–û–¥–Ω–æ –ø—Ä–æ–ø—É—â–µ–Ω–Ω–æ–µ —Å–ª–æ–≤–æ": find_one_gap,
        "–î–≤–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤–∞": find_two_gap,
        "–í–ª–æ–∂–µ–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã": find_nested
    }

    questions = []
    for key, func in extractors.items():
        sec = categorized.get(key, "")
        if not sec.strip():
            continue
        found = func(sec)
        for q in found:
            if isinstance(q, tuple):
                q_text = f"{q[0]}\n{q[1]}"
            else:
                q_text = str(q)
            questions.append(q_text.strip())

    return questions, None


def find_comp_desc(key, competencies):
    """–ò—â–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –ø–æ –∫–ª—é—á—É.
    –°—Ç—Ä–∞—Ç–µ–≥–∏—è: —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ -> –ø–æ–∏—Å–∫ –∫–ª—é—á–µ–π, –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö—Å—è —Å key -> –ø–æ–∏—Å–∫ –ø–æ —Ü–∏—Ñ—Ä–æ–≤–æ–π —á–∞—Å—Ç–∏ -> None
    """
    if key in competencies:
        return competencies[key]

    # –ü–æ–ø—ã—Ç–∫–∞ –Ω–∞–π—Ç–∏ –±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω—ã–µ –∫–ª—é—á–∏, –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è —Å –¥–∞–Ω–Ω–æ–≥–æ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –£–ö5.3 -> –£–ö5.3.1)
    candidates = [ (k,v) for k,v in competencies.items() if k.startswith(key) or key.startswith(k) ]
    if candidates:
        # –≤—ã–±–∏—Ä–∞–µ–º –Ω–∞–∏–±–æ–ª–µ–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π (—Å–∞–º—ã–π –¥–ª–∏–Ω–Ω—ã–π –∫–ª—é—á)
        best = max(candidates, key=lambda kv: len(kv[0]))
        return best[1]

    # –ü–æ–ø—ã—Ç–∫–∞ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø–æ —Ü–∏—Ñ—Ä–æ–≤–æ–π —á–∞—Å—Ç–∏: —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, 53 —Å 531)
    digits = re.sub(r"\D", "", key)
    if digits:
        for k,v in competencies.items():
            if digits and digits in re.sub(r"\D", "", k):
                return v

    return None


# ---------- –ì–ï–ù–ï–†–ê–¶–ò–Ø ----------
def extract_program_info(file_path):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏ –ø—Ä–æ—Ñ–∏–ª—å –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π"""
    full_text = docx2txt.process(file_path)
    direction = ""
    profile = ""

    # –ò—â–µ–º —Å—Ç—Ä–æ–∫—É –≤–∏–¥–∞: "–ø–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—é 09.03.01   –ò–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞ –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–∞—è —Ç–µ—Ö–Ω–∏–∫–∞, –ø—Ä–æ—Ñ–∏–ª—å - –≠–í–ú, –∫–æ–º–ø–ª–µ–∫—Å—ã, —Å–∏—Å—Ç–µ–º—ã –∏ —Å–µ—Ç–∏"
    match = re.search(
        r"–ø–æ\s+–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—é\s+([\d\.]+\s*[–ê-–Ø–∞-—èA-Zazl—ë–Å\s,]+?)\s*,?\s*–ø—Ä–æ—Ñ–∏–ª—å\s*[-‚Äì‚Äî]\s*([–ê-–Ø–∞-—èA-Zazl—ë–Å\s,]+)",
        full_text
    )
    if match:
        direction = match.group(1).strip()
        profile = match.group(2).strip()

    return direction, profile

def generate_files_per_discipline(user_dir, disciplines, competencies, questions):
    generated = []

    comp_file = os.path.join(user_dir, "competencies.docx")
    direction, profile = extract_program_info(comp_file)

    # --- –£–¥–∞–ª—è–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ –≤–∫—Ä–∞–ø–ª–µ–Ω–∏—è "–ì–æ–¥ –Ω–∞–±–æ—Ä–∞ ..." ---
    direction = re.sub(r"–≥–æ–¥[^\n]*", "", direction, flags=re.IGNORECASE).strip()
    profile = re.sub(r"–≥–æ–¥[^\n]*", "", profile, flags=re.IGNORECASE).strip()

    if not direction:
        direction = "–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–µ —É–∫–∞–∑–∞–Ω–æ"
    if not profile:
        profile = "–ü—Ä–æ—Ñ–∏–ª—å –Ω–µ —É–∫–∞–∑–∞–Ω"

    for disc in disciplines:
        doc = Document()
        question_counter = 1

        # --- –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å—Ç–∏–ª—å –¥–æ–∫—É–º–µ–Ω—Ç–∞ (Times New Roman, 14 pt) ---
        style = doc.styles['Normal']
        font = style.font
        font.name = 'Times New Roman'
        font.size = Pt(14)
        style.element.rPr.rFonts.set(qn('w:eastAsia'), 'Times New Roman')

        # --- –ù–∞–∑–≤–∞–Ω–∏–µ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã ---
        discipline_match = re.search(r"(–ë\d+[–ê-–ØA-Zazl–∞-—è—ë–Å0-9\s,\-‚Äì]+)", disc)
        discipline_name = discipline_match.group(1).strip() if discipline_match else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞"

        # --- –ò—â–µ–º –∫–æ–¥—ã –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π ---
        comp_codes = re.findall(r"((?:–£–ö|–û–ü–ö|–ü–ö)\s*\d+(?:\.\d+)*)", disc)
        if comp_codes:
            base = re.match(r"((?:–£–ö|–û–ü–ö|–ü–ö)\s*\d+)", comp_codes[0])
            short_comp_code = base.group(1).strip() if base else comp_codes[0]
        else:
            short_comp_code = "–ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è –Ω–µ —É–∫–∞–∑–∞–Ω–∞"

        # --- –®–∞–ø–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ ---
        doc.add_paragraph(f"–ó–∞–¥–∞–Ω–∏—è –¥–ª—è –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ {short_comp_code}")
        doc.add_paragraph(f"–ø–æ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–µ {discipline_name}")
        doc.add_paragraph(f"–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ {direction}")
        doc.add_paragraph(f"–ü—Ä–æ—Ñ–∏–ª—å {profile}")
        doc.add_paragraph()

        # --- –¢–∞–±–ª–∏—Ü–∞ —Å –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏ ---
        if comp_codes:
            table = doc.add_table(rows=len(comp_codes) + 1, cols=3)
            table.style = 'Table Grid'

            # –ó–∞–≥–æ–ª–æ–≤–∫–∏
            hdr_cells = table.rows[0].cells
            hdr_cells[0].text = "–ö–æ–¥ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏"
            hdr_cells[1].text = "–ö–æ–¥ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞"
            hdr_cells[2].text = "–ù–æ–º–µ—Ä–∞ –≤–æ–ø—Ä–æ—Å–æ–≤"

            for i, full_code in enumerate(comp_codes, start=1):
                row = table.rows[i].cells
                base_code = re.match(r"((?:–£–ö|–û–ü–ö|–ü–ö)\s*\d+)", full_code).group(1)
                row[0].text = base_code if i == 1 else ""
                row[1].text = full_code.replace(" ", "")
                row[2].text = f"{(i - 1) * 15 + 1}‚Äì{i * 15}"

            # –ü—Ä–∏–º–µ–Ω—è–µ–º —à—Ä–∏—Ñ—Ç Times New Roman 14 –∫–æ –≤—Å–µ–º —è—á–µ–π–∫–∞–º
            for row in table.rows:
                for cell in row.cells:
                    for p in cell.paragraphs:
                        for r in p.runs:
                            r.font.name = 'Times New Roman'
                            r._element.rPr.rFonts.set(qn('w:eastAsia'), 'Times New Roman')
                            r.font.size = Pt(14)
        else:
            table = doc.add_table(rows=3, cols=3)
            table.style = 'Table Grid'
            hdr_cells = table.rows[0].cells
            hdr_cells[0].text = "–ö–æ–¥ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏"
            hdr_cells[1].text = "–ö–æ–¥ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞"
            hdr_cells[2].text = "–ù–æ–º–µ—Ä–∞ –≤–æ–ø—Ä–æ—Å–æ–≤"
            table.rows[1].cells[0].text = short_comp_code
            table.rows[1].cells[1].text = f"{short_comp_code}.1"
            table.rows[1].cells[2].text = "1‚Äì15"
            table.rows[2].cells[1].text = f"{short_comp_code}.2"
            table.rows[2].cells[2].text = "16‚Äì30"

        doc.add_paragraph("\n")

        # --- –û—Å–Ω–æ–≤–Ω–∞—è —á–∞—Å—Ç—å: –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –∏ –≤–æ–ø—Ä–æ—Å—ã ---
        for uk in comp_codes:
            uk_key = uk.replace(" ", "")
            desc = find_comp_desc(uk_key, competencies)
            if desc:
                # desc —É–∂–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ '–£–ö 1.1 ‚Äî –æ–ø–∏—Å–∞–Ω–∏–µ'
                desc = re.sub(r"^" + re.escape(uk) + r"\s*[‚Äì-]?\s*", "", desc).strip()
                desc = desc.lstrip("‚Äî").strip()

                p = doc.add_paragraph()
                run = p.add_run(f"{uk} ‚Äî {desc}")
                run.bold = True
                p.alignment = 1

                doc.add_paragraph()

                selected = random.sample(questions, min(15, len(questions)))
                for q in selected:
                    doc.add_paragraph(f"{question_counter}. {q}")
                    question_counter += 1

                doc.add_paragraph("\n")

            else:
                p = doc.add_paragraph()
                p.add_run(f"‚ö†Ô∏è {uk} ‚Äî –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
                p.alignment = 1

        # --- –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª ---
        filename = re.sub(r"[^A-Za-z–ê-–Ø–∞-—è0-9]", "_", disc[:40]) + ".docx"
        file_path = os.path.join(user_dir, filename)
        doc.save(file_path)
        generated.append(file_path)

    return generated


def send_long_message(chat_id, text, parse_mode=None, reply_markup=None):
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —á–∞—Å—Ç—è–º–∏ (–±–µ–∑–æ–ø–∞—Å–Ω–æ –¥–ª—è Telegram)."""
    MAX = 3500
    # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ä–∞–∑–¥–µ–ª—ã
    paragraphs = text.split('\n\n')
    parts = []
    cur = ''
    for p in paragraphs:
        p = p.strip()
        if not p:
            continue
        candidate = (cur + '\n\n' + p) if cur else p
        if len(candidate) <= MAX:
            cur = candidate
            continue
        # candidate too big
        if cur:
            parts.append(cur)
            cur = ''
        # –µ—Å–ª–∏ –æ–¥–∏–Ω –ø–∞—Ä–∞–≥—Ä–∞—Ñ —Å–∞–º –ø–æ —Å–µ–±–µ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π ‚Äî —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Å—Ç—Ä–æ–∫–∞–º
        if len(p) <= MAX:
            cur = p
        else:
            lines = p.split('\n')
            cur2 = ''
            for ln in lines:
                ln = ln.strip()
                if not ln:
                    continue
                cand2 = (cur2 + '\n' + ln) if cur2 else ln
                if len(cand2) <= MAX:
                    cur2 = cand2
                else:
                    if cur2:
                        parts.append(cur2)
                    # –µ—Å–ª–∏ –æ–¥–Ω–∞ —Å—Ç—Ä–æ–∫–∞ –¥–ª–∏–Ω–Ω–µ–µ MAX ‚Äî —Ä–µ–∂–µ–º –µ—ë
                    if len(ln) > MAX:
                        for i in range(0, len(ln), MAX):
                            parts.append(ln[i:i+MAX])
                        cur2 = ''
                    else:
                        cur2 = ln
            if cur2:
                cur = cur2
    if cur:
        parts.append(cur)

    for i, part in enumerate(parts):
        rm = reply_markup if i == len(parts) - 1 else None
        bot.send_message(chat_id, part, parse_mode=parse_mode, reply_markup=rm)


if __name__ == "__main__":
    print("ü§ñ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω: –ø–æ–∏—Å–∫ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ –Ω–∞–π–¥–µ–Ω–Ω—ã–º –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞–º")
    bot.polling(none_stop=True)
